{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook builds a new or augments an existing spelling dictionary to correct possible misspellings of facility type keywords in a facility name column. \n",
    "\n",
    "The output columns include `Country`, `Word`, `Misspelling`, `Frequency`, `Score`.\n",
    "\n",
    "- `Country`: country name.\n",
    "- `Word`: the correct facility type keyword.\n",
    "- `Misspelling`: misspelling.\n",
    "- `Frequency`: frequency of the misspelling's appearance.\n",
    "- `Score`: similarity score between `Word` and `Misspelling`, scales from 0 to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "from symspellpy import SymSpell\n",
    "from itertools import islice\n",
    "from fuzzywuzzy import fuzz\n",
    "from ordered_set import OrderedSet\n",
    "from fuzzywuzzy import process\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data import\n",
    "# import dataset as df\n",
    "dataDir = r\"C:\\Users\\DUANYUEYUN\\Documents\\ArcGIS\\Projects\\GRID3\\Healthsites\"\n",
    "priority_countries = ['South Sudan', 'Mozambique', 'Namibia', 'Nigeria', 'Zambia',\n",
    "                      'Sierra Leone', 'Ghana',  'Burkina Faso', 'Ethiopia', 'Somalia',\n",
    "                     'Rwanda', 'Kenya', 'Zimbabwe', 'Democratic Republic of the Congo']\n",
    "\n",
    "dfs = []\n",
    "for i in range(len(priority_countries)):\n",
    "    country = priority_countries[i]\n",
    "    filename = country + '-node.shp'\n",
    "    path = os.path.join(dataDir, country, filename)\n",
    "    df = gpd.read_file(path)\n",
    "    df['country'] = country\n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs, axis=0)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "# import type dictionary as type_dict\n",
    "dataDir = r\"C:\\Users\\DUANYUEYUN\\Documents\\GRID3\\Health facilities\\Data\\Africa\"\n",
    "type_dict = pd.read_csv(dataDir + \"//type_dict_1210.csv\")\n",
    "\n",
    "# import existing spelling dictionary as old_spelling_dict (optional if APPEND is False)\n",
    "old_spelling_dict = pd.read_csv('C:\\\\Users\\\\DUANYUEYUN\\\\Documents\\\\GRID3\\\\Health facilities\\\\Data\\\\Africa\\\\Spelling dict\\\\spelling_dict_1210.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset as df\n",
    "\n",
    "# import type dictionary as type_dict\n",
    "\n",
    "# import existing spelling dictionary as old_spelling_dict (optional if APPEND is False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# facility name column\n",
    "FACILITY_NAME = 'name'\n",
    "# column name for pre-cleaned facility name\n",
    "CLEAN_NAME = 'clean_name'\n",
    "# country column\n",
    "COUNTRY_COL = 'country'\n",
    "# path to save the results\n",
    "SAVE_PATH = r\"C:\\Users\\DUANYUEYUN\\Documents\\GRID3\\Health facilities\\Data\\Africa\\Spelling dict\\spelling_dict_add_1213.csv\"\n",
    "# True if appending the results to an existing spelling dictionary\n",
    "APPEND = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there's any country that does not exist in type dictionary\n",
    "for c in df[COUNTRY_COL].unique():\n",
    "    if c.upper() not in type_dict['Country'].str.upper().unique():\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country names in the type dictionary:\n",
      "['Angola' 'Benin' 'Botswana' 'Burkina Faso' 'Burundi' 'Cameroon'\n",
      " 'Cape Verde' 'Central African Republic' 'Chad' 'Comoros' 'Congo'\n",
      " \"Cote d'Ivoire\" 'Democratic Republic of the Congo' 'Djibouti'\n",
      " 'Equatorial Guinea' 'Eritrea' 'Ethiopia' 'Gabon' 'Gambia' 'Ghana'\n",
      " 'Guinea' 'Guinea Bissau' 'Kenya' 'Lesotho' 'Liberia' 'Madagascar'\n",
      " 'Malawi' 'Mali' 'Mauritania' 'Mauritius' 'Mozambique' 'Namibia' 'Niger'\n",
      " 'Nigeria' 'Rwanda' 'Sao Tome and Principe' 'Senegal' 'Seychelles'\n",
      " 'Sierra Leone' 'Somalia' 'South Africa' 'South Sudan' 'Sudan' 'Tanzania'\n",
      " 'Togo' 'Uganda' 'Zambia' 'Zanzibar' 'Zimbabwe' 'eSwatini']\n"
     ]
    }
   ],
   "source": [
    "print(\"Country names in the type dictionary:\")\n",
    "print(type_dict['Country'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-cleaning on facility name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preclean(df, facility_name, clean_name):\n",
    "    \"\"\"Performs pre-cleaning on facility name, including \n",
    "    removing punctuations and \"\"\"\n",
    "    \n",
    "    # replace NAs with empty string ''\n",
    "    df[facility_name] = df[facility_name].fillna('')\n",
    "    \n",
    "    df[clean_name] = df[facility_name].str.strip()\\\n",
    "            .str.replace(\"  \", \" \")\\\n",
    "            .str.replace('.', ' ')\\\n",
    "            .str.replace(':', ' ')\\\n",
    "            .str.replace(\"'\", ' ')\\\n",
    "            .str.replace('\"', ' ')\\\n",
    "            .str.replace('[', ' ')\\\n",
    "            .str.replace(']', ' ')\\\n",
    "            .str.replace('[-_,/\\(\\);]', ' ')\\\n",
    "            .str.replace('&', ' and ')\\\n",
    "            .str.replace(\"  \", \" \")\\\n",
    "            .str.strip()\\\n",
    "            .str.replace('center', 'centre', case=False)\\\n",
    "            .str.replace('Polyclinique', 'Polyclinic', case=False)\\\n",
    "            .str.replace('Clinique', 'Clinic', case=False)\\\n",
    "            .str.replace('Geral', 'General', case=False)\\\n",
    "            .str.replace('Dispensaire', 'Dispensary', case=False)\\\n",
    "            .str.replace('Hopital', 'Hospital', case=False)\\\n",
    "            .str.replace('Hospitais', 'Hospital', case=False)\\\n",
    "            .str.replace(\"Urbain\", \"Urban\", case=False)\\\n",
    "            .str.replace(\"Distrital\", \"District\", case=False)\\\n",
    "            .str.replace('^hosp | hosp | hosp$|^hosp$', ' Hospital ', case=False)\\\n",
    "            .str.replace(\"  \", \" \")\\\n",
    "            .str.strip()\n",
    "    \n",
    "    # replace NAs in clean_name with empty string ''\n",
    "    df[clean_name] = df[clean_name].fillna('')\n",
    "    \n",
    "    # change emptry string in facility_name back to NA\n",
    "    df[facility_name] = df[facility_name].replace('', np.nan)\n",
    "\n",
    "    # remove accent marks\n",
    "    df[clean_name] = [unidecode.unidecode(n) for n in df[clean_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_misspellings(df, type_dict, country_name, clean_name, country_col, \n",
    "                          skip_spellings=[], min_length=6):\n",
    "    \n",
    "    \"\"\"\n",
    "    skip_spellings: list of spellings that should not be considered as misspellings and should be skipped.\n",
    "    min_length: minimum length of type keywords to check for possible misspellings.\n",
    "    \"\"\"\n",
    "    # obtain type dictionary for the country\n",
    "    type_dict_ctr = type_dict[type_dict['Country'].str.upper()==country_name.upper()]\n",
    "    # obtain country-specific type keywords\n",
    "    type_keywords = ' '.join(list(type_dict_ctr['Type'].str.lower())).split()\n",
    "    \n",
    "    # convert from list to set to remove repeating words, then convert to list again\n",
    "    type_keywords_all = list(set(type_keywords))\n",
    "    # keep only keywords with the minimum length\n",
    "    type_keywords_to_check = [word for word in type_keywords_all if len(word)>=min_length]\n",
    "    \n",
    "    # obtain dataset for the country\n",
    "    df_ctr = df[df[country_col].str.upper()==country_name.upper()]\n",
    "    # obtain a list of words that appear in precleaned names\n",
    "    names = ' '.join(list(df_ctr[~pd.isna(df_ctr[clean_name])][clean_name].str.lower())).split()\n",
    "\n",
    "    columns = ['Country', 'Word', 'Misspelling', 'Frequency', 'Score']\n",
    "    results = pd.DataFrame()\n",
    "    \n",
    "    for word in type_keywords_to_check:\n",
    "        # keep just words that start with the same letter as the type keyword\n",
    "        # and have length at least half of the length of the type keyword\n",
    "        # also remove the words that already appear in type keywords\n",
    "        start_char = word[0] # first letter\n",
    "        min_len = len(word)//2 # minimum length requirement\n",
    "        names_word = [name for name in names if name.startswith(start_char) \n",
    "                      and len(name)>min_len and name not in type_keywords_all]\n",
    "\n",
    "        # write the relevant words to a text file\n",
    "        filename = word+\".txt\"\n",
    "        file1 = open(filename,\"w\")\n",
    "        file1.write(' '.join(names_word))\n",
    "        file1.close() \n",
    "\n",
    "        # generate word frequency dictionary\n",
    "        sym_spell = SymSpell()\n",
    "        sym_spell.create_dictionary(filename)\n",
    "        freq_dict = sym_spell.words\n",
    "        # remove the text file\n",
    "        os.remove(filename)\n",
    "\n",
    "        # compute similarity score with respect to the original word\n",
    "        \n",
    "        threshold = (len(word)-1) / len(word) # score threshold\n",
    "        for spelling, frequency in freq_dict.items():\n",
    "            if spelling in skip_spellings:\n",
    "                continue\n",
    "            ratio = fuzz.ratio(spelling, word)\n",
    "            if ratio/100>=threshold:\n",
    "                new_row = pd.DataFrame([[country_name, word, spelling, frequency, ratio]], columns=columns)\n",
    "                results = pd.concat([results, new_row])\n",
    "    #results = results[results['Score']<100]\n",
    "    if results.shape[0]>0:\n",
    "        results['Country'] = results['Country'].str.upper()\n",
    "    # reset and drop index\n",
    "    results.reset_index(inplace=True, drop=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using type dictionary, find the list of facility type keywords to check for misspellings.\n",
    "\n",
    "    - `min_length`: minimum length of keywords required, default=6.\n",
    "\n",
    "\n",
    "2. For each type keyword, obtain a list of word candidates that might be misspellings and then transform the list into a frequency dictionary.\n",
    "\n",
    "    - words that appear in precleaned names.\n",
    "    - words that start with the same first letter as the type keyword. \n",
    "    - words with length that is at least half of the length of the type keyword.\n",
    "    - words that do not appear in type keywords.\n",
    "\n",
    "\n",
    "3. For each word candidate, compute similarity score with the target type keyword.\n",
    "\n",
    "If the similarity score is greater than a threshold, add the word as a misspelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-cleaning on facility names\n",
    "preclean(df, facility_name = FACILITY_NAME, clean_name = CLEAN_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "South Sudan\n",
      "Mozambique\n",
      "Namibia\n",
      "Nigeria\n",
      "Zambia\n",
      "Sierra Leone\n",
      "Ghana\n",
      "Burkina Faso\n",
      "Ethiopia\n",
      "Somalia\n",
      "Rwanda\n",
      "Kenya\n",
      "Zimbabwe\n",
      "Democratic Republic of the Congo\n"
     ]
    }
   ],
   "source": [
    "spelling_dict = pd.DataFrame()\n",
    "\n",
    "for country_name in df[COUNTRY_COL].unique():\n",
    "    print(country_name)\n",
    "    # generate misspellings for that country\n",
    "    country_results = generate_misspellings(df, type_dict, country_name, clean_name=CLEAN_NAME,\n",
    "                                           country_col=COUNTRY_COL) \n",
    "    # merge country results to all results\n",
    "    spelling_dict = pd.concat([spelling_dict, country_results])\n",
    "# reset and drop index\n",
    "spelling_dict.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39, 5)\n"
     ]
    }
   ],
   "source": [
    "print(spelling_dict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Word</th>\n",
       "      <th>Misspelling</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MOZAMBIQUE</td>\n",
       "      <td>centro</td>\n",
       "      <td>centros</td>\n",
       "      <td>2</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MOZAMBIQUE</td>\n",
       "      <td>centro</td>\n",
       "      <td>centrode</td>\n",
       "      <td>2</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NIGERIA</td>\n",
       "      <td>patent</td>\n",
       "      <td>patient</td>\n",
       "      <td>1</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NIGERIA</td>\n",
       "      <td>university</td>\n",
       "      <td>univeraity</td>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NIGERIA</td>\n",
       "      <td>federal</td>\n",
       "      <td>federeal</td>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Country        Word Misspelling  Frequency  Score\n",
       "0  MOZAMBIQUE      centro     centros          2     92\n",
       "1  MOZAMBIQUE      centro    centrode          2     86\n",
       "2     NIGERIA      patent     patient          1     92\n",
       "3     NIGERIA  university  univeraity          1     90\n",
       "4     NIGERIA     federal    federeal          1     93"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spelling_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of new rows added: 29\n"
     ]
    }
   ],
   "source": [
    "if APPEND:\n",
    "    # add new rows to existing spelling dictionary\n",
    "    new_spelling_dict = pd.concat([old_spelling_dict, spelling_dict])\n",
    "    new_spelling_dict = new_spelling_dict.groupby(['Country', 'Word', 'Misspelling'])\\\n",
    "    .agg(Frequency=('Frequency', 'sum'), Score=('Score', 'mean'))\n",
    "    new_spelling_dict.reset_index(inplace=True)\n",
    "    print(\"Number of new rows added:\", new_spelling_dict.shape[0]-old_spelling_dict.shape[0])\n",
    "    new_spelling_dict.to_csv(SAVE_PATH, index=False)\n",
    "else:\n",
    "    # if not append, just write the results into a .csv file\n",
    "    spelling_dict.to_csv(SAVE_PATH, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
